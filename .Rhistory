# Import data from social_norms.xls
rm(list=ls())
library(tidyverse)
library(wordcloud)
library(tm)
library(RWeka)
setwd("../")
# read data
master <- readxl::read_xlsx(path = "Social Norms meta.xlsx", sheet = "ALL")
authors_attributes <- readxl::read_xlsx("authors.xlsx") %>% distinct(id, .keep_all = T) %>% separate(Id_paper_affil,sep = ":", into=c("PaperId","Other"))
master_methods <- readxl::read_excel("Social Norms meta.xlsx", sheet = "ALL") %>%
subset.data.frame(subset = Method_elicitation =="KW" |
Method_elicitation =="Bicchieri" |
Method_elicitation =="Both") %>%
distinct(PaperID, .keep_all = T) %>% subset.data.frame(select = c("PaperID", "Authors", "Year", "Outlet", "Published", "Game_type", "Method_elicitation"))
master_methods
View(master_methods)
# text analysis -------------
ords <- master %>% group_by(PaperID) %>% arrange(PaperID) %>%filter(row_number()==1)%>% ungroup() %>% select(Keywords) %>% sapply(str_split, pattern=";") %>% unlist()
docs <- VCorpus(VectorSource(ords))
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
#docs <- tm_map(docs, removeNumbers)
# Remove your own stop word
# specify your stopwords as a character vector
#docs <- tm_map(docs, removeWords, c('"'))
# Remove punctuations
#docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
#docs <- tm_map(docs, stripWhitespace)
#newBigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
dtm <- TermDocumentMatrix(docs, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words=200, random.order=FALSE, rot.per=0.1,
colors=brewer.pal(8, "Dark2"))
title("Alternative")
d
dtm
docs
View(d)
View(master)
